{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Jack:\n",
    "    def __init__(self):\n",
    "        self.move_limit = 5 # Can be in [-5, 5]\n",
    "        self.gamma = 0.9 # Discount rate\n",
    "        self.states_in_every_store = 21 ## {0 car, 1 car, ..., 20 car}\n",
    "        self.car_limit = 20\n",
    "        self.V = np.zeros((self.states_in_every_store, self.states_in_every_store))\n",
    "        self.Policy = np.zeros((self.states_in_every_store, self.states_in_every_store))\n",
    "        #self.Policy = np.random.randint(low=-5,high=5,size=(21,21))\n",
    "        \n",
    "    def greedy_action(self, obs):\n",
    "        loc1_car_no, loc1_returned_yesterday, loc2_car_no, loc2_returned_yesterday = obs\n",
    "        loc1_car_no += loc1_returned_yesterday ## Adding the returned cars considering agent's actions\n",
    "        if loc1_car_no > self.car_limit:\n",
    "            loc1_car_no = self.car_limit\n",
    "        loc2_car_no += loc2_returned_yesterday\n",
    "        if loc2_car_no > self.car_limit:\n",
    "            loc2_car_no = self.car_limit\n",
    "        return self.Policy[int(loc1_car_no), int(loc2_car_no)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Jacks_Rental_problem.jpeg\" width=900 hight=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Location:\n",
    "    def __init__(self, car_no, returned_yesterday, request_lambda, return_lambda):\n",
    "        self.car_no = car_no\n",
    "        self.request_lambda = request_lambda\n",
    "        self.return_lambda = return_lambda\n",
    "        self.returned_yesterday = returned_yesterday\n",
    "        \n",
    "class Business:\n",
    "    def __init__(self):\n",
    "        self.loc1 = Location(car_no=20,returned_yesterday=0,request_lambda=3,return_lambda=3)\n",
    "        self.loc2 = Location(car_no=20,returned_yesterday=0,request_lambda=4,return_lambda=2)\n",
    "        self.state = self.loc1.car_no, self.loc1.returned_yesterday, self.loc2.car_no, self.loc2.returned_yesterday\n",
    "        \n",
    "        self.car_limit = 20\n",
    "        self.profit_renting = 10\n",
    "        self.moving_car_price = -2\n",
    "    @staticmethod\n",
    "    def poisson_number_gen(self, lam, n=20):\n",
    "        poisson_mass = []\n",
    "        for i in range(0,n):\n",
    "            prob = math.pow(lam, i) * math.exp(-lam)/ math.factorial(i)\n",
    "            poisson_mass.append(prob)\n",
    "        var, pp, u = 0, poisson_mass[0], np.random.uniform(0, 1)\n",
    "        while(u>pp):\n",
    "            var += 1\n",
    "            pp = pp + poisson_mass[var]\n",
    "        return var\n",
    "    \n",
    "    def reset(self):\n",
    "        self.loc1.car_no = 20\n",
    "        self.loc2.car_no = 20\n",
    "        self.loc1.returned_yesterday = 0\n",
    "        self.loc2.returned_yesterday = 0\n",
    "        loc1_req = Business.poisson_number_gen(lam=self.loc1.request_lambda)\n",
    "        self.loc1.car_no -= loc1_req\n",
    "        loc2_req = Business.poisson_number_gen(lam=self.loc2.request_lambda)\n",
    "        self.loc2.car_no -= loc2_req\n",
    "        self.loc1.returned_yesterday = Business.poisson_number_gen(lam=self.loc1.return_lambda)\n",
    "        self.loc2.returned_yesterday = Business.poisson_number_gen(lam=self.loc2.return_lambda)\n",
    "        self.state = self.loc1.car_no, self.loc1.returned_yesterday, self.loc2.car_no, self.loc2.returned_yesterday\n",
    "        return self.state\n",
    "    @staticmethod\n",
    "    def general_step(init_state, action):\n",
    "        \"\"\"\n",
    "        Actual state is a combination of the init_state and the generated random requests and returns.\n",
    "        The problem is that state cannot be pre-determined as it was the case for the GridWorld environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def step(self, action): ## Actions={-5,-4,...,-1,0,1,...,4,5}\n",
    "        self.loc1.car_no += (self.loc1.returned_yesterday + action) ## Adding the returned cars considering agent's actions\n",
    "        if self.loc1.car_no > self.car_limit:\n",
    "            self.loc1.car_no = self.car_limit\n",
    "        if self.loc1.car_no < 0:\n",
    "            self.loc1.car_no = 0\n",
    "        self.loc2.car_no += (self.loc2.returned_yesterday - action)\n",
    "        if self.loc2.car_no > self.car_limit:\n",
    "            self.loc2.car_no = self.car_limit\n",
    "        if self.loc2.car_no < 0:\n",
    "            self.loc2.car_no = 0\n",
    "            \n",
    "        loc1_req = self.poisson_number_gen(lam=self.loc1.request_lambda)\n",
    "        self.loc1.car_no -= loc1_req\n",
    "        if self.loc1.car_no < 0:\n",
    "            self.loc1.car_no = 0\n",
    "            \n",
    "        print(\"loc1_req:\", loc1_req)\n",
    "        loc2_req = self.poisson_number_gen(lam=self.loc2.request_lambda)\n",
    "        self.loc2.car_no -= loc2_req\n",
    "        if self.loc2.car_no < 0:\n",
    "            self.loc2.car_no = 0\n",
    "        \n",
    "        self.loc1.returned_yesterday = self.poisson_number_gen(lam=self.loc1.return_lambda)\n",
    "        self.loc2.returned_yesterday = self.poisson_number_gen(lam=self.loc2.return_lambda)\n",
    "        \n",
    "        self.state = self.loc1.car_no, self.loc1.returned_yesterday, self.loc2.car_no, self.loc2.returned_yesterday\n",
    "        reward = (loc1_req * self.profit_renting) + (loc2_req * self.profit_renting) - abs(action * self.moving_car_price)\n",
    "        done = False\n",
    "        if self.loc1.car_no <= 0 or self.loc2.car_no <= 0:\n",
    "            done = True\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Ch04-PolicyIter.png\" width=600 hight=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Episode 1 ======\n",
      "Init State: (18, 4, 15, 2)\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(17, 1, 15.0, 6) 50.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(15.0, 4, 13, 2) 100.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(16.0, 4, 10.0, 1) 80.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(17.0, 2, 6.0, 3) 80.0\n",
      "0.0\n",
      "loc1_req: 7\n",
      "(12.0, 5, 8.0, 2) 80.0\n",
      "0.0\n",
      "loc1_req: 0\n",
      "(17.0, 2, 8.0, 1) 20.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(16.0, 1, 8.0, 1) 40.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(13.0, 2, 6.0, 2) 70.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(12.0, 4, 1.0, 0) 100.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(12.0, 6, -4.0, 2) 90.0\n",
      "Episode_Reward: 710.0\n",
      "====== Episode 2 ======\n",
      "Init State: (14, 2, 14, 4)\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(14.0, 3, 11.0, 5) 90.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(14.0, 4, 12.0, 4) 70.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(16.0, 1, 13.0, 1) 50.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(15.0, 0, 12.0, 4) 40.0\n",
      "0.0\n",
      "loc1_req: 1\n",
      "(14.0, 4, 12.0, 2) 50.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(15.0, 1, 12.0, 1) 50.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(12.0, 1, 7.0, 2) 100.0\n",
      "0.0\n",
      "loc1_req: 1\n",
      "(12.0, 0, 6.0, 0) 40.0\n",
      "0.0\n",
      "loc1_req: 5\n",
      "(7.0, 5, 5.0, 1) 60.0\n",
      "0.0\n",
      "loc1_req: 1\n",
      "(11.0, 0, 3.0, 0) 40.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(9.0, 3, -5.0, 4) 100.0\n",
      "Episode_Reward: 690.0\n",
      "====== Episode 3 ======\n",
      "Init State: (20, 3, 14, 4)\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(17, 3, 14.0, 2) 70.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(16.0, 2, 15.0, 2) 50.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(16.0, 2, 15.0, 2) 40.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(14.0, 3, 15.0, 1) 60.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(14.0, 3, 12.0, 3) 70.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(14.0, 2, 14.0, 2) 40.0\n",
      "0.0\n",
      "loc1_req: 1\n",
      "(15.0, 8, 12.0, 1) 50.0\n",
      "0.0\n",
      "loc1_req: 0\n",
      "(20, 6, 10.0, 2) 30.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(16, 3, 9.0, 1) 70.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(16.0, 2, 8.0, 2) 50.0\n",
      "0.0\n",
      "loc1_req: 5\n",
      "(13.0, 4, 4.0, 2) 110.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(13.0, 3, 1.0, 0) 90.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(12.0, 4, -4.0, 1) 90.0\n",
      "Episode_Reward: 820.0\n",
      "====== Episode 4 ======\n",
      "Init State: (19, 0, 16, 3)\n",
      "0.0\n",
      "loc1_req: 6\n",
      "(13.0, 2, 12.0, 6) 130.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(12.0, 4, 15.0, 4) 60.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(14.0, 4, 15.0, 1) 60.0\n",
      "0.0\n",
      "loc1_req: 5\n",
      "(13.0, 5, 13.0, 0) 80.0\n",
      "0.0\n",
      "loc1_req: 6\n",
      "(12.0, 4, 10.0, 0) 90.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(14.0, 3, 9.0, 1) 30.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(13.0, 2, 8.0, 4) 60.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(12.0, 5, 8.0, 2) 70.0\n",
      "0.0\n",
      "loc1_req: 6\n",
      "(11.0, 3, 4.0, 2) 120.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(12.0, 0, 3.0, 0) 50.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(10.0, 3, 2.0, 1) 30.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(10.0, 4, -2.0, 4) 80.0\n",
      "Episode_Reward: 860.0\n",
      "====== Episode 5 ======\n",
      "Init State: (15, 1, 16, 4)\n",
      "0.0\n",
      "loc1_req: 5\n",
      "(11.0, 4, 14.0, 3) 110.0\n",
      "0.0\n",
      "loc1_req: 4\n",
      "(11.0, 1, 14.0, 4) 70.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(9.0, 1, 14.0, 2) 70.0\n",
      "0.0\n",
      "loc1_req: 5\n",
      "(5.0, 4, 14.0, 0) 70.0\n",
      "0.0\n",
      "loc1_req: 1\n",
      "(8.0, 4, 9.0, 3) 60.0\n",
      "0.0\n",
      "loc1_req: 0\n",
      "(12.0, 5, 9.0, 1) 30.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(14.0, 1, 6.0, 2) 70.0\n",
      "0.0\n",
      "loc1_req: 2\n",
      "(13.0, 3, 6.0, 1) 40.0\n",
      "0.0\n",
      "loc1_req: 3\n",
      "(13.0, 3, -1.0, 2) 110.0\n",
      "Episode_Reward: 630.0\n"
     ]
    }
   ],
   "source": [
    "def areDeltasLargerThanTheta(d, t, while_counter):\n",
    "    if while_counter < 2: ## Turning the while to a do..while loop\n",
    "        return True\n",
    "    for i in range(d.shape[0]):\n",
    "        if abs(d[i]) > abs(t):\n",
    "            return True\n",
    "    return False\n",
    "#def iterative_policy_evaluation():\n",
    "if __name__ == \"__main__\":\n",
    "    world = Business()\n",
    "    agent = Jack()\n",
    "    theta = 1e-3 ## Every element in delta will be compared to this value\n",
    "    V_s = np.zeros((20,20))\n",
    "    deltas = np.zeros((20,20)) # [0, 0 ...]\n",
    "    while_count = 0\n",
    "    while areDeltasLargerThanTheta(deltas, theta, while_count):\n",
    "        ################\n",
    "        k = while_count\n",
    "        if k == 0 or k == 1 or k == 2 or k == 3 or k % 10 == 0:\n",
    "            print(\"K =\", k)\n",
    "            print(\"--------\")\n",
    "            print(world.worldValues)\n",
    "        ################\n",
    "        deltas = np.zeros((20,20)) # [0, 0 ...] Zeroing deltas every time\n",
    "        while_count+=1\n",
    "        for state in range(1,15):\n",
    "            v = world.get_value(state) ## v <- V(s)\n",
    "            new_v = 0\n",
    "            for action in range(len(world.actions)):                   ## Generating the \\sum \\pi(a|s)\n",
    "            #This is an undicounted update (no \\gamma)\n",
    "                new_s, r = GridWorld.general_step(state, action)       ## p(s',r | s,a) \n",
    "                new_v += agent.policy() * (r + world.get_value(new_s)) ## Adding the value of every action to the value of the state\n",
    "            world.set_value(state, new_v)                              ## V(s) <- Expression\n",
    "            deltas[state] = max(deltas[state], abs(new_v - v))         ## \\delta <- max(\\delta, |v - V(s)|)\n",
    "print(\"Number of main loops, Last K =\", while_count)\n",
    "print(\"--------\")\n",
    "print(world.worldValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
